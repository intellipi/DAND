{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Enron Project: Free-Response Questions\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "***\n",
    "\n",
    "> Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it.  \n",
    "As part of your answer, give some background on the dataset and how it can be used to answer the project question.   \n",
    "Were there any outliers in the data when you got it, and how did you handle those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project goal was to use machine learning techniques over e-mails and corporate financial data to classify Enron employees into two groups: those involved in the financial scandal, named POIs (persons of interest), and those not. It leverages on computational power and statistical algorithms to identify patterns in known cases to label unknown ones into a group or another. \n",
    "\n",
    "To accomplish the task, both financial and e-mail information were summarized by person's name. A list of POIs was manually generated based on individuals who were indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity. There were total 21 features in the dataset: 14 financial features, 6 email features and the 'POI' feature, the one to be tested against the model. \n",
    "\n",
    "The dataset contains 146 data points, each representing one Enron employee. Of those, 18 were flagged as POIs and 128 as non-POIs.   \n",
    "\n",
    "Of the 146 data points (persons), 3 outliers were removed: two that are clearly not persons ('TOTAL' and 'THE TRAVEL AGENCY IN THE PARK') and another with all values equal to zero ('LOCKHART EUGENE E'), which could bring mislead results to the model.   \n",
    "\n",
    "Before running the models, the features values were checked for missing information. The aim was to avoid bias due to 'NaN' concentrated in POIs.   \n",
    "For example, if all POIs have 'NaN' for a specific feature, the algorithm may conclude that \"every time a feature is missing, the person is a POI\".  \n",
    "\n",
    "Two features were dropped because of 'NaN' issue: `restricted_stock_deferred` and `director_fees`. The `email` feature was also dropped, because there's nothing that can be predicted from it.\n",
    "\n",
    "The next step was to create 2 new features to be tested: \n",
    "1. `to_poi_ratio`: the percentage of messages sent to a POI / total messages sent; \n",
    "2. `from_poi_ratio`: the percentage of messages received from a POI / total messages received.\n",
    "In theory those involved in the fraud should be working more closely together.  \n",
    "\n",
    "The list of the 19 features in the dataset, ordered by k-best scores (from best to worst, after rescaling) are as follows:  \n",
    "\n",
    "|#   \t|Feature   \t|K-Best Score   \t|\n",
    "|---\t|---\t|---\t|\n",
    "|1.   \t|exercised_stock_options   \t|24.815079733218194   \t|\n",
    "|2.   \t|total_stock_value   \t|24.182898678566872   \t|\n",
    "|3.   \t|bonus   \t|20.792252047181538   \t|\n",
    "|4.   \t|salary   \t|18.289684043404513   \t|\n",
    "|5.   \t|deferred_income   \t|11.458476579280697   \t|\n",
    "|6.   \t|long_term_incentive   \t|9.9221860131898385   \t|\n",
    "|7.   \t|restricted_stock   \t|9.212810621977086   \t|\n",
    "|8.   \t|total_payments   \t|8.7727777300916809   \t|\n",
    "|9.   \t|shared_receipt_with_poi   \t|8.5894207316823774   \t|\n",
    "|10.   \t|loan_advances   \t|7.1840556582887247   \t|\n",
    "|11.   \t|expenses   \t|6.0941733106389666   \t|\n",
    "|12.   \t|from_poi_to_this_person   \t|5.2434497133749574   \t|\n",
    "|13.   \t|from_poi_ratio   \t|5.1239461527568899   \t|\n",
    "|14.   \t|other   \t|4.1874775069953785   \t|\n",
    "|15.   \t|to_poi_ratio   \t|4.0946533095769446   \t|\n",
    "|16.   \t|from_this_person_to_poi   \t|2.3826121082276743   \t|\n",
    "|17.   \t|to_messages   \t|1.6463411294420094   \t|\n",
    "|18.   \t|deferral_payments   \t|0.22461127473600509   \t|\n",
    "|19.   \t|from_messages   \t|0.16970094762175436   \t|\n",
    "\n",
    "\n",
    "From the above feature list we conclude that financial information (except for 'other') is better than e-mail information for predicting POIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "***\n",
    "\n",
    "> What features did you end up using in your POI identifier, and what selection process did you use to pick them?   \n",
    "Did you have to do any scaling? Why or why not?  \n",
    "\n",
    "> As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.)  \n",
    "\n",
    "> In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline was used to garantee all steps were run in sequence over the same data sample. The steps were:\n",
    "1. Scaling using MinMaxScaler\n",
    "2. PCA transformation (all models tested with and without this step)\n",
    "2. K-Best selection\n",
    "4. Classifier (Gaussian Naive Bayes, Decision Tree, Adaboost and Random Forest)\n",
    "\n",
    "The features were rescaled using MinMaxScaler (except for 'POI') because financial ones, expressed in USD, had a much wider range of values compared to emails sent/received, which would cause model distortion depending on the classifier used. \n",
    "\n",
    "All classifiers were tested using k-best method, with k equal to [4, 6, 8 and 10]. We ended up using 4 features, after grid searching all parameters combinations: `exercised_stock_options`, `total_stock_value`, `bonus` and `salary`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "***\n",
    "\n",
    "> What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tested classifiers were Gaussian Naive Bayes, Decision Tree, Adaboost and Random Forest.   \n",
    "Gaussian Naive Bayes was the classifier of choice.\n",
    "\n",
    "Best results for each classifiers were:\n",
    "***  \n",
    "\n",
    "|**Classifier**   \t        |**PCA (Y/N)** \t|**Accuracy**   \t|**Precision**   \t|**Recall**   \t|**f1**   \t|\n",
    "|---\t                    |---\t        |---\t            |---\t            |---\t        |---\n",
    "|**Gaussian NB**   \t            |**No**   \t        |**0.847**  \t        |**0.412**   \t        |**0.329**          |**0.366**   \t|\n",
    "|Decision Tree              |No   \t        |0.840   \t        |0.340  \t        |0.215 \t        |0.263   \t|\n",
    "|Adaboost   \t            |No   \t        |0.824   \t        |0.294   \t        |0.227 \t        |0.256   \t|\n",
    "|Random Forest \t            |No   \t        |0.849   \t        |0.372   \t        |0.188          |0.250   \t|\n",
    "|.....      \t            |..... \t        |.....   \t        |.....   \t        |.....          |.....   \t|\n",
    "|Gaussian NB   \t            |Yes   \t        |0.812  \t        |0.312   \t        |0.343          |0.327   \t|\n",
    "|Decision Tree              |Yes   \t        |0.810   \t        |0.275  \t        |0.258 \t        |0.266   \t|\n",
    "|Adaboost   \t            |Yes   \t        |0.820   \t        |0.291   \t        |0.245 \t        |0.266  \t|\n",
    "|Random Forest \t            |Yes   \t        |0.837   \t        |0.331   \t        |0.218          |0.263   \t|\n",
    "\n",
    "\n",
    "Conclusions:\n",
    "- Gaussian NB was the best classifier for the problem in hand, the only to pass the minimum bar of 0.3 for both precision and recall.\n",
    "- PCA preprocessing didn't influence the results significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "***\n",
    "\n",
    "> What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  \n",
    "How did you tune the parameters of your particular algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means to test the algorithm against a wide range of possible combinations in search for the best solution on the train set. If it's not properly done, the model will unecessarily return wrong predictions when going live.  A model's efficiency is measued by how well it performs on unseen data. It may overfit or underfit in what is known as the bias-variance tradeoff. \n",
    "\n",
    "Overfitting (high variance, low bias) occurs when the model overreacts to minor fluctuations in the training data, incorporating a large noise component.   \n",
    "Underfitting (low variance, high bias) happens when the model is overly simplified, unable to capture important trends. \n",
    "\n",
    "Algorithm parameters are not solely responsible for model assertiveness and speed. Depending on the input data and number of features available, it's necessary (or recommended) to preprocess before loading it: feature rescaling (i.e MinMax Rescaling), selection (i.e k-best), dimensional space reduction (i.e PCA) and other techniques which may facilitate the algorithms predicting power and reduce their potential for a complex fit.\n",
    "\n",
    "GridsearchCV was the method used to systematically work through the multiple possible combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance. \n",
    "\n",
    "The following parameters were tested on the models:  \n",
    "\n",
    "Gaussian NB:\n",
    "    * PCA: [True, False]\n",
    "    * k-best: [4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "Decision Tree:\n",
    "    * PCA: [True, False]\n",
    "    * k-best: [4, 6, 8, 10]\n",
    "    * criterion: [\"entropy\", \"gini\"]\n",
    "    * min_samples_leaf: [2, 4, 6]\n",
    "    * min_samples_split: [2, 4, 6]\n",
    "    \n",
    "Adaboost:\n",
    "    * PCA: [True, False]\n",
    "    * k-best: [4, 6, 8, 10]\n",
    "    * n_estimators: [40, 50, 60]\n",
    "    * learning_rate: [.6, .8, 1, 1.2, 1.5]\n",
    "\n",
    "Random Forest:\n",
    "    * PCA: [True, False]\n",
    "    * k-best: [4, 6, 8, 10]\n",
    "    * criterion: [\"entropy\", \"gini\"]\n",
    "    * min_samples_leaf: [1, 2, 4]\n",
    "    * min_samples_split: [2, 4, 6]\n",
    "    * n_estimators: [5, 10, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "***\n",
    "\n",
    "> What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation is the process of running the chosen algorithm against unseen data to verify model performance.   \n",
    "A classic mistake is loading test data into the model for training, giving it a perfect fit but returning poor results when going live on unseen data.\n",
    "\n",
    "Our analysis was cross-validated using `train_test_split` technique, which splits arrays or matrices into random train and test subsets. 80% of data points were randomly selected to train the model, while the remaining 20% were kept to test it. In sequence, it was tested using `StratifiedShuffleSplit` (SSS). The SSS cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "***\n",
    "\n",
    "> Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Score:  \n",
    "(True Positives + True Negatives) / Total Cases.  \n",
    "Model accuracy score: 0.847  \n",
    "It means 84.7% of the time the model predicted true or false correctly. Of all people, 15.3% would either answer for a crime while being not guilty or walk away with it if guilty.\n",   
    "\n",
    "\n",
    "Precision Score:  \n",
    "True Positives / (True Positives + False Positives)  \n",
    "Model precision score:  0.412  \n",
    "It means for every time the model predicted a person was a POI, 41.2% of the time he/she actually was.\n",
    "\n",
    "\n",
    "Recall Score:  \n",
    "True Positives / (True Positives + False Negatives)  \n",
    "Model recall score: 0.329  \n",
    "It means for every existing POI, 32.9% of the time the model correctly classified him/her as a POI. Solely using the model by this criteria, 67.1% of POIs would walk away with it.   "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
